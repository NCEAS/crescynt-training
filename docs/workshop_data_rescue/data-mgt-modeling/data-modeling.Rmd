---
title: "Managing and Structuring your Data"
time_slot: 60 minutes
output:
  ioslides_presentation:
    wide: "true"
    logo: "images/NCEAS_logo_small.png"
---

```{r echo = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Managing your data


## The Need for Data Management: Data Deluge 

```{r fig.align="center"}
knitr::include_graphics("images/Deluge.png")
```


## Benefits for Your Own Research

- Keep yourself organized
- Avoid data loss
- Better control versions of data
- Format and document your data for your own recollection, accountability, and re-use
- Gain credibility and recognition for your science efforts through data sharing!
- Capture your science processes to make them reproducible


## Benefits for the Scientific Community

- Data is a valuable asset 
- Data should be managed to:
  - Maximize the effective use and value of data
  - Continually improve the quality (accuracy, integrity, integration,...) 
  - Ensure appropriate use of data and information
  - Facilitate data sharing 
  - Ensure sustainability and accessibility in long term for re-use in science


## Data Life Cycle

The data life cycle provides a high level overview of the stages involved in successful management and preservation of data for use and reuse.

```{r}
knitr::include_graphics("images/DLC.png")
```


## Data Management Plans (DMP)

A data management plan describes how you will manage your data during the lifetime of a research project.


## Why doing a DMP?

For any scientific project, it is good practice to prepare a data management plan (DMP). The process of creating your DMP will force you to think about potential issues related to the project's data that could affect timeline, costs and personnel needed.

>- It might be required by founding agency
>- It will help you in your project planning and resources allocations
>- It will help you to share and promote your work (Publications, Data citation, ...)


## Don't loose your data

*Accidents happen !!!*

![](images/broken-laptop.png) ![](images/screen-keyboard.png)


## Document: Sooner the Better

Document and preserve your data when you are actively analyzing them! 

```{r fig.cap = "Adapted from Michener et al, 1997"}
knitr::include_graphics("images/DLC.png")
```

<div class="notes">
At this time, it is the easiest for you to document and preserve your project’s data, because you know and understand the most about them. Then slowly, or more rapidly due to life events and technology shifts, what you know about the data declines.
</div>


## Document Soon, it is also for yourself {.build}

You would not have to remember:

- The name of that file?
- The directory where you put it?
- The units those measurements were taken in?
- Which sample site was which?
- Is it the cleaned version of the data set used for publication?

=> Easier to share with others, good for collaborations!


## Not only Data

We mainly have being talking about data, but these rules apply to all the scientific processes and products generated by a research project, including:

- Your scientific workflow
- Your scripts developed to manipulate and analyse the data
- The models the tools you used or developed


# Data Modeling for Data Reuse

## Learning Objectives

- Understand basics of data models
- Learn how to design and create effective data tables

## Benefits of normalized or "tidy" data {.build}

- Powerful search and filtering
- Handle large and/or complex data sets
- Help to enforce data integrity
- Easier to handle data updates

*=> Easier to conduct your Analysis and even so for others!!*


## Data Heterogeneity

Data are heterogeneous in:

>- _Structure_ (schema): Logical model of the data (e.g., tables, hierarchical trees, raster images, etc.)
>- _Semantics_: Specific meaning of the data (e.g., nature and types of measurements, importance of contextual information, interpretation of record structure, etc.): documentation
>- _Syntax_: Digital format of the data (e.g., csv, “R data frame”, NetCDF, Excel XLSX, DBF, etc.)


## Why Tabular Data?

Spreadsheets are (still) the primary data entry tool of the digital age!

```{r}
knitr::include_graphics("images/spreadsheet-1.png")
```


## Spreadsheets

### ![](images/theGood.png) __the Good__:

- Quick on the draw (clickety-click and you’re ready for action)
- Always there (on most everyone’s computer)
- Smarter than he lets on (stats, pivot tables, VB scripts)
- Cleans up real pretty (graphics, fonts, colors, borders)


## Spreadsheets

### ![](images/theBad.png) __the Bad__:

- Also a fast shooter  (click&fire)
- No scruples (delete row, click, ctrl-x/ctrl-c, re-sort, save)
- Talks a good story, but unreliable (e.g. http://www.practicalstats.com/xlsstats/excelstats.html)


## Spreadsheets

### ![](images/theUgly.png) __the Ugly__:

- Ill-mannered: takes your data prisoner; conflates raw data with summary data
- Gaudy: Use of visual formatting to indicate critical metadata or other semantic tidbits
- Shifty: Cross-linking of worksheets sets up “invisible” dependencies
- The more complicated your Spreadsheet, the UGLIER it gets in terms of using it with other software


## Spreadsheets

### *_=>  Encourages you to mix your data and your analaysis_*


## Data Organization

```{r out.width = "90%"}
knitr::include_graphics("images/excel-org-01.png")
```


## Multiple tables

```{r out.width = "90%"}
knitr::include_graphics("images/excel-org-02.png")
```


## Inconsistent observations

```{r out.width = "90%"}
knitr::include_graphics("images/excel-org-03.png")
```


## Inconsistent variables

```{r out.width = "90%"}
knitr::include_graphics("images/excel-org-04.png")
```


## Marginal sums and statistics

```{r out.width = "90%"}
knitr::include_graphics("images/excel-org-05.png")
```

*=>  A Spreadsheet is not a table !!*


# Good enough data modeling

## Terminological Soup

Table = Relation = Data set   (~ Worksheet) 

Column = Variable = Attribute = Characteristic

Row = Record = Tuple  <> Observation

Keys are used to Join or Merge

Cell = Value = Measurement 

Data Model = Schema


## Denormalized data (aka non-tidy)

Observations about different entities combined

```{r out.width = "90%"}
knitr::include_graphics("images/table-denorm.png")
```

<div class="notes">
In the above example, each row has measurements about both the `site` at which observations occurred, as well as observations of two individuals of possibly different species found at that site.  This is *not normalized* data.

People often refer to this as *wide* format, because the observations are spread across a wide number of columns.  Note that, should one encounter a new species in the survey, we wold have to add new columns to the table.  This is difficult to analyze, understand, and maintain.
</div>

## Tabular data

__Observations__. A better way to model data is to organize the observations about each type of entity in its own table.  This results in:

- Separate tables for each type of entity measured
- Each row represents a single observed entity
- Observations (rows) are all unique

__This is *normalized* data (aka *tidy data*)__


## Tabular data

__Variables__. In addition, for normalized data, we expect the variables to be organized such that:

- All values in a column are of the same type
- All columns pertain to the same observed entity (e.g., row)

## Tabular data

```{r out.width = "90%"}
knitr::include_graphics("images/tables-norm.png")
```

<div class="notes">
Here's an example of tidy (normalized) data in which the top table is the collection
of observations about individuals of several species, and the bottom table are the
observations containing properties of the sites at which the species occurred.
</div>

## How to relate tables?

When one has normalized data, we often use unique identifiers to reference
particular observations, which allows us to link across tables.  Two types of
identifiers are common within relational data:

- _Primary Key_: unique identifier for each observed entity, one per row
- _Foreign Key_: reference to a primary key in another table (linkage)


## How to relate tables?

```{r out.width = "90%"}
knitr::include_graphics("images/tables-keys.png")
```

<div class="notes">
For example, in the second table below, the `site` column is the *primary key* 
of that table, because it uniquely identifies each row of the table as a unique
observation of a site.  In the first table, however, the `site` column is a 
*foreign key* that references the primary key from the second table.  This linkage
tells us that the first height measurement for the `DAPU` observation occurred
at the site with the name `Taku`.
</div>


## Entity-Relationship Model (ER)

An Entity-Relationship model allows us to compactly draw the structure of the
tables in a relational database, including the primary and foreign keys in the tables.

```{r}
knitr::include_graphics("images/plotobs-diagram.png")
```

In the above model, one can see that each site in the `SITES` table must have one
or more observations in the `PLOTOBS` table, whereas each `PLOTOBS` has one and 
only one `SITE`.


## Simple Guidelines for Effective Data

- Design to add rows, not columns
- Each column one type
- Header line
- Non-proprietary formats
- Descriptive names
- No spaces


## Semantic Ambiguity

- Column headers: 
   - Avoid cryptic names
   - Concise, but not meaningful 
   - Units (kg or g?) 
- Color coding:
   - avoid using formatting (implicit)
   - add a column to store this information with a flag

## Semantic Ambiguity

```{r}
knitr::include_graphics("images/semantics-color.png")
```

## Semantic Ambiguity

```{r}
knitr::include_graphics("images/semantics-legend.png")
```

## Semantic Ambiguity

```{r}
knitr::include_graphics("images/semantics-good.png")
```

## Semantic Ambiguity

```{r}
knitr::include_graphics("images/semantics-right.png")
```


## Resources used

- Recknagel, F., Michener, W.K., 2018. Ecological informatics: data management and knowledge discovery, 3rd ed. ed. Springer, Cham.
- DataONE, Data Life Cycle: https://www.dataone.org/data-life-cycle
- DataONE data management guide: https://www.dataone.org/sites/all/documents/DataONE-PPSR-DataManagementGuide.pdf
- ESIP, Data Management Plans: http://commons.esipfed.org/datamanagementshortcourse (benefits slides were adapted from this material)
- Borer, Elizabeth T., Eric W. Seabloom, Matthew B. Jones, and Mark Schildhauer. (2009) "Some Simple Guidelines for Effective Data Management." The Bulletin of the Ecological Society of America 90, no. 2: 205-14. https://doi.org/10.1890/0012-9623-90.2.205.


## Resources used

- Michener, W. K. (2015). Ten Simple Rules for Creating a Good Data Management Plan. PLoS Comput Biol , 11(10). presented at the 10/2015.  https://doi.org/10.1371/journal.pcbi.1004525
- [Borer et al. 2009. **Some Simple Guidelines for Effective Data Management.** Bulletin of the Ecological Society of America.](http://matt.magisa.org/pubs/borer-esa-2009.pdf)
- [Software Carpentry SQL tutorial](https://swcarpentry.github.io/sql-novice-survey/)
- [Tidy Data](http://vita.had.co.nz/papers/tidy-data.pdf)

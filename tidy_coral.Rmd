---
title: "Tidy Coral"
output: html_document
---

## Introduction

We are going to spend the morning beginning a tidy coral analysis that you will continue on your own in the afternoon. We'll be using what we learned in the [data-science-training](http://ohi-science.org/data-science-training/) but with real coral data from Oahu. 

Our plan is to combine benthic observation data with buoy oceanographic data and explore patterns. 

The benthic data comes from digital still images through the [Hawaii Coral Reef Assessment and Monitoring Program (CRAMP)](https://search.dataone.org/#view/{21548C09-57F9-449A-86B2-470A1DD4F0B5}), and the buoy data is from the [Kaneohe Bay NOAA buoy at Coconut Island (Station MOKH1)](http://www.ndbc.noaa.gov/station_history.php?station=mokh1).

We'll also be using a few new R packages that are super helpful: `stringr`, `janitor`, and `skimr`. 

## Setup

First off, let's open up a GitHub repo we were working in yesterday, and start a new RMarkdown file. I'll call mine tidy_coral_analysis.Rmd. 

I'll add a tiny bit of information to get us started, and you can fill in once you know more about what the analysis becomes: 

> Exploratory analysis to combine benthic observation data with buoy oceanographic data.

Then we'll add a setup chunk:

```{r setup, warning=FALSE, message=FALSE}
## libraries
library(tidyverse)
library(stringr)
library(janitor) # install.packages('janitor')
library(skimr) # install.packages('skimr')

## data locations ---

## benthic data
benthic_filepath <- 'data/100308OaAla03m.CSV'

## buoy data
buoy_url <- 'http://www.ndbc.noaa.gov/view_text_file.php?filename=mokh1h2010.txt.gz&dir=data/historical/stdmet/'
```

## Benthic data

This is benthic data from a series of CRAMP (Coral Reef Assessment Monitoring Program) data that includes KBay coral survey still images and extracted data (with larger Hawaiian Islands context):

[2015. Hawaii Coral Reef Assessment and Monitoring Program (CRAMP): benthic data from digital still images made in 2008-2010 on Kauai, Oahu, Molokai, Maui, and Hawaii (NODC Accession 0104255). NOAA NCEI Oceanographic Data Archive.](https://search.dataone.org/#view/{6A218D8B-05CD-49EA-AC13-FD0372B3B1D4}) 

We have to navigate to [here](https://www.nodc.noaa.gov/archive/arc0054/0104255/1.1/data/0-data/cd05/Oahu/OaHan/) and download it; I've saved it in the /data folder.

### Import

```{r read_csv benthic, message=FALSE}
benthic_raw <- read_csv(benthic_filepath)
head(benthic_raw) 
```

There is a lot of columns that are all NA, but let's not worry about that right now. 

### Wrangle

Let's use the janitor package to clean up the column headers. We can actually just create this new `benthic` object through our pipe here: 
```{r janitor and skimr}
## the `janitor` package's `clean_names` function
benthic <- benthic_raw %>%
  janitor::clean_names()

head(benthic)
```
`janitor::clean_names()` is such a useful function!

Let's pull out a few columns that look useful for working with and go from there. 
```{r select benthic}
benthic <- benthic %>%
  select(id_name, point, x, y, id_date)

head(benthic)
```
Great. But let's have another look at those dates. There are some weird `#`s leading and trailing the dates that will surely cause trouble later, and they don't look good. So let's remove them. We can create another column called simply "date". 

```{r}
benthic <- benthic %>%
  mutate(date = str_remove_all(id_date, "#"))
```

### Explore

Now let's have a quick look at some summary stats:
```{r skim benthic}
summary(benthic)
skimr::skim(benthic)
```
`skimr::skim()` lets us see quickly that there are 6 unique dates and 16 unique species. It will also make cool histograms of continueous data, although we won't focus on that at the moment. 

Let's check out which species are represented. 

```{r unique species}
unique(benthic$id_name)
```

And let's just have a quick plot of species count by date: 

```{r plot benthic}
ggplot(benthic, aes(id_date, fill = id_name)) +
         geom_bar()
```
OK so there would be a lot of ways we could improve this plot. But we just wanted a quick look. And this will help us frame our questions later. For example: 

- why do total counts increase so much through time?

Great! Let's leave this for a moment and read in the other data. 

## Buoy data

### Import

Let's also have a look in the Environment pane as we read in the data.

```{r read_csv, message=FALSE}
d <- readr::read_csv(buoy_url)
head(d) # hmm this doesn't look right! Why not?
```

Why didn't that import properly? Let's have a look at the URL of the data. Ah right. It's a .txt, not a .csv. 

OK. `readr` should have a function to read in .txt files. Let's navigate to the help menu and have a look. In the bottom right RStudio pane, click on "Packages". Type `readr` in the search menu, then click on its name. This will let you see all of the functions within the package. 

There are a lot of options, but let's try `read_table`. It will return a dataframe.

```{r read_table, message=FALSE}
## read_table
d_raw <- read_table(buoy_url)
head(d) ## still not quite right -- missed some columns. 

## read_table2
d_raw <- read_table2(buoy_url)
head(d_raw) ## this is what we need!
```

In creating this tutorial, I actually tried a few other options that didn't work for various reasons. But cool to see them and why they didn't work:

```{r read_delim, message=FALSE}
## this just wasn't the right approach
d_test <- read_delim(buoy_url, delim = " ", trim_ws = TRUE, skip=1)
```

This involved way too many steps, and saving a temporary copy of the data (unideal!)
```{r read_lines, message=FALSE, warning=FALSE}
d_test <- read_lines(buoy_url)
y <- d_test %>%
  as_data_frame() %>%
  mutate(value = str_replace_all(value, ' +', ',')) 
write_delim(y, 'data/buoy_local_copy.csv')

z <- read_csv('data/buoy_local_copy.csv', skip=1)
## PRAISE BE
```

Cool. Nice that `read_table2` was designed to get the job done.

### Copy `d_raw` as a new variable

We've got `d_raw` as the raw data we read from online. Let's create a new variable called `d` that we'll wrangle instead of that raw data (especially nice if you've got poor internet and don't want to read it in each time!)
```{r}
d <- d_raw
```

### Column Headers

Let's look at the column headers. 
```{r, echo=FALSE}
names(d)
```

We've got two rows of information about what the data represent. And actually, one of those rows is actually the column headers, and R thinks that the second is data. Let's clean up those names. But we don't want to lose either of those rows, because they both have important and unique information (measurement and units). 

So, let's see if we can take that the first row of data (the units) and stick it on the with the column names (measurement). Then, we can get rid of that units row. 

In the `stringr` package, there is a way to combine strings using the `str_c` function. Let's go back up to the setup and add `stringr` to the setup chunk and run it. 

There's 3 things we want to do to these column names: 

1. make the column header a combo of rows 1 and 2
1. clean up the header; get rid of `#` and `/`
1. delete the now-redundant row 1

```{r clean col names}
## 1. overwrite column names
names(d) <- str_c(names(d),                     ## first thing to combine
                  d %>% filter(`#YY` == "#yr"), ## second thing
                  sep = "_")                    ## separate these two things by this

## inspect
names(d) ## Looks a lot better


## 2. clean up a bit more to get rid of the `#`s and the `/`s. 
names(d) <- str_replace_all(names(d), "#", "")  # let's just get rid of # all together (replace with "")
names(d) <- str_replace_all(names(d), "/", "_") # but let's replace / with _

## inspect to make sure it worked
names(d)
```

Cool, that looks good enough for now!

Now, let's remove that first row that still displays the units. Note that since it is the first line, we could do this by deliberately saying line 1, but I don't want to do that because I could easily and silently start deleting rows of data if I ran these lines over and over. 

```{r}
## inspect dimensions
dim(d)

## 3. remove redundant row with units
d <- d %>%
  filter(YY_yr != "#yr")

## inspect again
dim(d)
head(d)

## and what would happen if we reran this? 
d <- d %>%
  filter(YY_yr != "#yr")

## inspect again -- no change!
dim(d)
head(d)
```

### Next steps

What are some other things we could do to this data? 

- all of these values are strings instead of numbers (because of that `#yr` row). We could change them so we can treat them as numbers (math) instead of words
- combine the date columns
- missing data: what does 99.0 mean? How about 999?

### Format dates

Right now the dates are across several columns. Let's combine them into dates. 

```{r change to numeric}
## first change to numeric
d <- d %>%
  mutate(YY_yr = as.numeric(YY_yr), 
         MM_mo = as.numeric(MM_mo), 
         DD_dy = as.numeric(DD_dy))
         #hh_hr = as.numeric(hh_hr),
         #mm_mn = as.numeric(mm_mn))

## pad zeros (add this after join attempt below)
d <- d %>%
  mutate(MM_mo = str_pad(MM_mo, 2, "left", pad = 0),
         DD_dy = str_pad(DD_dy, 2, "left", pad = 0))

```


```{r unite}
d <- d %>%
  unite(date, c(YY_yr, MM_mo, DD_dy), sep = "-")
```

Let's say we want to look at these species compared to temperature. 
What could we join these datasets on?

Date. 

Looks like we'll have to clean up the date column first


## Join datasets

```{r}
d_join <- benthic %>%
  left_join(d, by = "date")
```

Why didn't it work? Have a look at the dates in each dataframe specifically: 

```{r}
head(d$date)
head(benthic$date)
```

There aren't zeros in the `d` object. Let's go back up to the date part of our analysis and add them.

Hooray!

We've got a lot of observations; maybe we actually want to go back and summarize a bit before we join, or do some summaries now. What should we try next? It's up to you...

## Your turn

### Ideas: 

#### Explore benthic data

- what other ways could you visualize the data? What questions does bring up?


#### Explore buoy data

#### Explore joined data

#### Explore beyond

- Compare a different buoy and benthic pair?

---
title: "Tidy Coral"
output: html_document
---

Here we're going to use what we learned yesterday on coral datasets. 

We'll combine buoy data (temperature)

We'll also be using a few other packages that are super helpful: `stringr`, `janitor`, and `skimr`. 


We will do a bit together to get you started and then leave it to you to ask the questions you want to ask with these data.

First off, let's setup: 

```{r setup, warning=FALSE, message=FALSE}
## libraries
library(tidyverse)
library(stringr)
library(janitor) # install.packages('janitor')

## buoy data url
buoy_url <- 'http://www.ndbc.noaa.gov/view_text_file.php?filename=mokh1h2010.txt.gz&dir=data/historical/stdmet/'

cramp_filepath <- 'data/100308OaAla03m.CSV'

#https://data.nodc.noaa.gov/cgi-bin/iso?id=gov.noaa.nodc:0117490

```

## Buoy data

### Import

While this is always the first step, it's not always a straight-forward step. What functions should we use? 

Let's also have a look in the Environment pane as we read in the data.

```{r read_csv, message=FALSE}
d <- readr::read_csv(buoy_url)
head(d) # hmm this doesn't look right! Why not?
```

Why didn't that install properly? Let's have a look at the URL of the data. Ah right. It's a .txt, not a .csv. 

OK. `readr` should have a function to read in .txt files. Let's navigate to the help menu and have a look. In the bottom right RStudio pane, click on "Packages". Type `readr` in the search menu, then click on its name. This will let you see all of the functions within the package. 

There are a lot of options, but let's try `read_table`. It will return a dataframe.

```{r read_table, message=FALSE}
## read_table
d_raw <- read_table(buoy_url)
head(d) ## still not quite right -- missed some columns. 

## read_table2
d_raw <- read_table2(buoy_url)
head(d_raw) ## this is what we need!
```

In creating this tutorial, I actually tried a few other options that didn't work for various reasons. But cool to see them and why they didn't work:

```{r read_delim, message=FALSE}
## this just wasn't the right approach
d_test <- read_delim(buoy_url, delim = " ", trim_ws = TRUE, skip=1)
```

This involved way too many steps, and saving a temporary copy of the data (unideal!)
```{r read_lines, message=FALSE, warning=FALSE}
d_test <- read_lines(buoy_url)
y <- d_test %>%
  as_data_frame() %>%
  mutate(value = str_replace_all(value, ' +', ',')) 
write_delim(y, 'data/buoy_local_copy.csv')

z <- read_csv('data/buoy_local_copy.csv', skip=1)
## PRAISE BE
```

Cool. Nice that `read_table2` was designed to get the job done.

### Copy `d_raw` as a new variable

We've got `d_raw` as the raw data we read from online. Let's create a new variable called `d` that we'll wrangle instead of that raw data (especially nice if you've got poor internet and don't want to read it in each time!)
```{r}
d <- d_raw
```

### Column Headers

Let's look at the column headers. 
```{r, echo=FALSE}
names(d)
```

We've got two rows of information about what the data represent. And actually, one of those rows is actually the column headers, and R thinks that the second is data. Let's clean up those names. But we don't want to lose either of those rows, because they both have important and unique information (measurement and units). 

So, let's see if we can take that the first row of data (the units) and stick it on the with the column names (measurement). Then, we can get rid of that units row. 

In the `stringr` package, there is a way to combine strings using the `str_c` function. Let's go back up to the setup and add `stringr` to the setup chunk and run it. 

There's 3 things we want to do to these column names: 

1. make the column header a combo of rows 1 and 2
1. clean up the header; get rid of `#` and `/`
1. delete the now-redundant row 1

```{r clean col names}
## 1. overwrite column names
names(d) <- str_c(names(d),                     ## first thing to combine
                  d %>% filter(`#YY` == "#yr"), ## second thing
                  sep = "_")                    ## separate these two things by this

## inspect
names(d) ## Looks a lot better


## 2. clean up a bit more to get rid of the `#`s and the `/`s. 
names(d) <- str_replace_all(names(d), "#", "")  # let's just get rid of # all together (replace with "")
names(d) <- str_replace_all(names(d), "/", "_") # but let's replace / with _

## inspect to make sure it worked
names(d)
```

Cool, that looks good enough for now!

Now, let's remove that first row that still displays the units. Note that since it is the first line, we could do this by deliberately saying line 1, but I don't want to do that because I could easily and silently start deleting rows of data if I ran these lines over and over. 

```{r}
## inspect dimensions
dim(d)

## 3. remove redundant row with units
d <- d %>%
  filter(YY_yr != "#yr")

## inspect again
dim(d)
head(d)

## and what would happen if we reran this? 
d <- d %>%
  filter(YY_yr != "#yr")

## inspect again -- no change!
dim(d)
head(d)
```

### Next steps

What are some other things we could do to this data? 

- all of these values are strings instead of numbers (because of that `#yr` row). We could change them so we can treat them as numbers (math) instead of words
- combine the date columns
- missing data: what does 99.0 mean? How about 999?

### Format dates

Right now the dates are across several columns. Let's combine them into dates. 

```{r change to numeric}
## first change to numeric
d <- d %>%
  mutate(YY_yr = as.numeric(YY_yr), 
         MM_mo = as.numeric(MM_mo), 
         DD_dy = as.numeric(DD_dy))
         #hh_hr = as.numeric(hh_hr),
         #mm_mn = as.numeric(mm_mn))

## pad zeros (add this after join attempt below)
d <- d %>%
  mutate(MM_mo = str_pad(MM_mo, 2, "left", pad = 0),
         DD_dy = str_pad(DD_dy, 2, "left", pad = 0))

```


```{r unite}
d <- d %>%
  unite(date, c(YY_yr, MM_mo, DD_dy), sep = "-")
```

Great! Let's leave this for a moment and read in some other data. 

## Benthic data

and benthic data from digital stills 

https://search.dataone.org/#view/{6A218D8B-05CD-49EA-AC13-FD0372B3B1D4}

https://www.nodc.noaa.gov/archive/arc0054/0104255/1.1/data/0-data/
https://www.nodc.noaa.gov/archive/arc0054/0104255/1.1/data/0-data/cd05/Oahu/OaHan/



Series of CRAMP (Coral Reef Assessment Monitoring Program) data that includes KBay coral survey still images and extracted data (with larger Hawaiian Islands context):

2015. Hawaii Coral Reef Assessment and Monitoring Program (CRAMP): benthic data from digital still images made in 2008-2010 on Kauai, Oahu, Molokai, Maui, and Hawaii (NODC Accession 0104255). NOAA NCEI Oceanographic Data Archive. 

We have to navigate to this and download it; I've saved it in the /data folder.
```{r read_csv benthic, message=FALSE}
ben_raw <- read_csv(cramp_filepath)
head(ben_raw) # much more straight-forward!
```

There is a lot of columns that are all NA, but let's not worry about that right now. 

Let's use the janitor package to clean up the column headers. We can actually just create this new `benthic` object through our pipe here: 
```{r janitor and skimr}
## the `janitor` package's `clean_names` function
benthic <- ben_raw %>%
  janitor::clean_names()

summary(benthic)
# skim(benthic)
```

Let's pull out a few columns that we'll be working with and go from there. 
```{r select benthic}
benthic <- benthic %>%
  select(id_name, point, x, y, id_date)

summary(benthic)
# skim(benthic)
```

Let's check out which species are represented. 

```{r bentic species}
unique(benthic$id_name)
```

Let's say we want to look at these species compared to temperature. 
What could we join these datasets on?

Date. 

Looks like we'll have to clean up the date column first

```{r}
benthic <- benthic %>%
  mutate(date = str_remove_all(id_date, "#"))
```

Great!

## Join datasets

```{r}
d_join <- benthic %>%
  left_join(d, by = "date")
```

Why didn't it work? Have a look at the dates in each dataframe specifically: 

```{r}
head(d$date)
head(benthic$date)
```

There aren't zeros in the `d` object. Let's go back up to the date part of our analysis and add them.

Hooray!

We've got a lot of observations; maybe we actually want to go back and summarize a bit before we join, or do some summaries now. What should we try next? It's up to you...

---
title: "Tidy Coral"
output: html_document
---

Here we're going to use what we learned yesterday on coral datasets. 

We'll combine buoy data (temperature)

We'll also be using a few other packages that are super helpful: `stringr`, `janitor`, and `skimr`. 


We will do a bit together to get you started and then leave it to you to ask the questions you want to ask with these data.

First off, let's setup: 

```{r setup, warning=FALSE, message=FALSE}
## libraries
library(tidyverse)
# library(magrittr) ## %<>%
library(stringr)

buoy_url <- 'http://www.ndbc.noaa.gov/view_text_file.php?filename=mokh1h2008.txt.gz&dir=data/historical/stdmet/'

#https://data.nodc.noaa.gov/cgi-bin/iso?id=gov.noaa.nodc:0117490

```

## Buoy data

### Import

While this is always the first step, it's not always a straight-forward step. What functions should we use? 

Let's also have a look in the Environment pane as we read in the data.

```{r read_csv, message=FALSE}
d <- readr::read_csv(buoy_url)
head(d) # hmm this doesn't look right! Why not?
```

Why didn't that install properly? Let's have a look at the URL of the data. Ah right. It's a .txt, not a .csv. 

OK. `readr` should have a function to read in .txt files. Let's navigate to the help menu and have a look. In the bottom right RStudio pane, click on "Packages". Type `readr` in the search menu, then click on its name. This will let you see all of the functions within the package. 

There are a lot of options, but let's try `read_table`. It will return a dataframe.

```{r read_table, message=FALSE}
## read_table
d_raw <- read_table(buoy_url)
head(d) ## still not quite right -- missed some columns. 

## read_table2
d_raw <- read_table2(buoy_url)
head(d_raw) ## this is what we need!
```

In creating this tutorial, I actually tried a few other options that didn't work for various reasons. But cool to see them and why they didn't work:

```{r read_delim, message=FALSE}
## this just wasn't the right approach
d_test <- read_delim(buoy_url, delim = " ", trim_ws = TRUE, skip=1)
```

This involved way too many steps, and saving a temporary copy of the data (unideal!)
```{r read_lines, message=FALSE, warning=FALSE}
d_test <- read_lines(buoy_url)
y <- d_test %>%
  as_data_frame() %>%
  mutate(value = str_replace_all(value, ' +', ',')) 
write_delim(y, 'data/buoy_local_copy.csv')

z <- read_csv('data/buoy_local_copy.csv', skip=1)
## PRAISE BE
```

Cool. Nice that `read_table2` was designed to get the job done.

### Copy `d_raw` as a new variable

We've got `d_raw` as the raw data we read from online. Let's create a new variable called `d` that we'll wrangle instead of that raw data (especially nice if you've got poor internet and don't want to read it in each time!)
```{r}
d <- d_raw
```

### Column Headers

Let's look at the column headers. 
```{r, echo=FALSE}
names(d_raw)
```

We've got two rows of information about what the data represent. And actually, one of those rows is actually the column headers, and R thinks that the second is data. Let's clean up those names. But we don't want to lose either of those rows, because they both have important and unique information (measurement and units). 

So, let's see if we can take that the first row of data (the units) and stick it on the with the column names (measurement). Then, we can get rid of that units row. 

In the `stringr` package, there is a way to combine strings using the `str_c` function. Let's go back up to the setup and add `stringr` to the setup chunk and run it. 

There's 3 things we want to do to these column names: 

1. make the column header a combo of rows 1 and 2
1. clean up the header; get rid of `#` and `/`
1. delete the now-redundant row 1

```{r clean col names}
## 1. overwrite column names
names(d) <- str_c(names(d),                     ## first thing to combine
                  d %>% filter(`#YY` == "#yr"), ## second thing
                  sep = "_")                    ## separate these two things by this

## inspect
names(d) ## Looks a lot better


## 2. clean up a bit more to get rid of the `#`s and the `/`s. 
names(d) <- str_replace_all(names(d), "#", "")  # let's just get rid of # all together (replace with "")
names(d) <- str_replace_all(names(d), "/", "_") # but let's replace / with _

## inspect to make sure it worked
names(d)
```

Cool, that looks good enough for now!

Now, let's remove that first row that still displays the units. Note that since it is the first line, we could do this by deliberately saying line 1, but I don't want to do that because I could easily and silently start deleting rows of data if I ran these lines over and over. 

```{r}
## inspect dimensions
dim(d)

## 3. remove redundant row with units
d <- d %>%
  filter(YY_yr != "#yr")

## inspect again
dim(d)
head(d)

## and what would happen if we reran this? 
d <- d %>%
  filter(YY_yr != "#yr")

## inspect again -- no change!
dim(d)
head(d)
```

### Next steps

What are some other things we could do to this data? 

- all of these values are strings instead of numbers (because of that `#yr` row). We could change them so we can treat them as numbers (math) instead of words
- combine the date columns
- missing data: what does 99.0 mean? How about 999?

### Format dates

Right now the dates are across several columns. Let's combine them into dates. 

```{r change to numeric}
## first change to numeric
d <- d %>%
  mutate(YY_yr = as.numeric(YY_yr), 
         MM_mo = as.numeric(MM_mo), 
         DD_dy = as.numeric(DD_dy))
         #hh_hr = as.numeric(hh_hr),
         #mm_mn = as.numeric(mm_mn))
```


```{r unite}
dtmp <- d %>%
  unite(date, c(YY_yr, MM_mo, DD_dy), sep = "-")
```



Great! Let's leave this for a moment and read in some other data. 

Series of CRAMP (Coral Reef Assessment Monitoring Program) data that includes KBay coral survey still images and extracted data (with larger Hawaiian Islands context):


```{r read in cramp data}
# cramp <- read_csv('data/Maalaea_Deep.csv')
# head(cramp) # much more straight-forward!

## TODO: Janitor

## Let's have a look at the summary
# skim(cramp)
```

What I see: 

- there are some weird columns named "X Data 1", 2 and 3 that are all missing. Probably some relic from Excel? Or might be worth inspection
- otherwise, data are pretty complete
- there are 9 unique species in this dataset
- X, Y coordinated do not look like they are lat/lon.

Let's check out which species are represented. 

```{r cramp species}

# unique(cramp$Species)

```

Let's say we want to look at these species compared to temperature. 
What could we join these datasets on?


## On your own

1. Learn what the "X Data 1", 2 and 3 columns are and remove them if not necessary
